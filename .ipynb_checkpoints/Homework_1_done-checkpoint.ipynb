{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T11:23:02.789502500Z",
     "start_time": "2023-12-16T11:23:02.090131Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T11:23:21.684685800Z",
     "start_time": "2023-12-16T11:23:16.692914900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T11:23:26.279023500Z",
     "start_time": "2023-12-16T11:23:24.781976100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8315608779817632e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:05:12.461060100Z",
     "start_time": "2023-12-13T08:05:12.398549700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55005566973676e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-6.82527137e-09,  3.03660232e-08,  1.57889417e-08, -6.25363696e-08,\n        8.30524440e+01, -9.17433197e-08, -9.02600414e-09,  2.41782082e-08,\n        4.55144589e+01,  2.88017604e-09, -3.95703194e-08,  2.42588701e-08,\n        9.64414128e-12, -1.76189604e-08,  7.81311350e+00,  2.25730838e-08,\n       -3.21202512e-08, -5.26081814e-08,  2.68098991e-08,  1.56934046e-08,\n        4.75687270e-08, -2.62666139e-08, -1.64191655e-08, -1.00479037e-08,\n        2.33869442e+01,  1.48225450e-08,  1.84002365e-09, -2.34728054e-08,\n       -5.99347199e-08,  1.46591792e+01,  3.63790699e+01, -5.39194700e-08,\n       -5.98970727e-08,  1.50608792e+01,  1.14690662e-08,  3.13734443e-08,\n       -3.36365691e-08,  2.96531758e-08, -1.29044987e-08,  9.31701294e-09,\n        1.85232594e-08, -7.33297529e-09, -1.90397237e-08,  5.90751375e-08,\n        8.16362175e-09,  1.41066893e-08, -3.81794468e-08, -2.68920818e-08,\n        2.32462916e-08,  4.64030883e-08,  3.08431201e-08, -8.11529612e-09,\n       -1.19403404e-08,  2.91543270e-08, -1.86605233e-08,  8.53918831e+01,\n        1.94792410e-08, -3.09822935e-08, -2.11798020e-08,  1.94137526e-08,\n       -2.42127140e-08, -2.24614049e-09, -2.25413247e-08, -5.48631174e-08,\n       -6.95969825e-09,  7.18331545e+01,  4.41631694e-08, -5.97643785e-08,\n       -4.35746011e-08,  7.51958427e-09,  4.30811547e-08, -9.45703879e-09,\n        2.73684317e-08,  4.41951523e-08,  4.61565862e-08,  3.31919917e-09,\n       -3.43363155e-08, -6.41764489e-09,  1.48767819e+01,  4.22586663e-08,\n       -1.86101488e-08,  1.26245725e-08,  2.05514734e-08,  3.48802299e-08,\n        8.25815558e-09,  1.72406858e-09, -6.61182302e-09, -2.33870734e-08,\n        3.53438286e-08, -4.45301935e-08, -4.88755705e-08, -5.41503392e-08,\n       -3.98709015e-08,  6.83270768e-08, -6.28777534e-08, -7.84733684e-09,\n        7.52895507e-09, -2.00697167e-08,  2.60736676e-08, -2.88064104e-08])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "Эта метрика измеряет, насколько средние квадратичные отклонения предсказанных значений от фактических. Для этой задачи, чем ближе значение MSE к нулю, тем лучше модель.\n",
    "\n",
    "Различие в значениях MSE между LinearRegression и SGDRegressor может быть связано с несколькими факторами. Во-первых, гиперпараметр alpha в SGDRegressor отвечает за силу регуляризации, и его неоптимальное значение может влиять на результаты. Во-вторых, стохастический характер SGD и неявное указание количества итераций могут привести к случайным колебаниям в результатах.\n",
    "\n",
    "Для улучшения результатов с SGDRegressor, стоит экспериментировать с гиперпараметрами, такими как alpha и max_iter, и подбирать их значения для достижения близких к значениям MSE, полученным с LinearRegression.\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression.\n",
    "\n",
    "Для подбора гиперпараметров в методе градиентного спуска (SGDRegressor), мы можем использовать процесс подбора по сетке (Grid Search), который позволяет перебирать различные комбинации значений гиперпараметров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Linear Regression: 2.915847048486403e-25\n",
      "MSE SGDRegressor: 5.1349624624302236e-12\n",
      "Coefficients SGDRegressor: [ 8.69257510e-08 -3.17855243e-09 -1.31149344e-08 -3.54923540e-08\n",
      "  1.25653037e-08 -4.98720565e-08 -3.48188341e-08 -1.74038464e-08\n",
      " -4.77537521e-08  4.14228970e-08 -3.60210735e-08  8.67127026e-09\n",
      "  5.44353497e+01  1.09901372e+01 -8.40612838e-09 -5.09053890e-09\n",
      " -1.83678241e-08  4.05788927e-08 -1.08353588e-08  3.43299619e-08\n",
      "  2.53406350e-08 -2.08454887e-09 -6.08318577e-09  3.80640608e-08\n",
      " -6.53364319e-08 -6.36968220e-08  3.63489150e+00  2.06400550e-08\n",
      "  6.88175157e-10 -7.40713779e-08  5.02615377e-08  6.60917905e-08\n",
      " -3.75644954e-08  1.69626462e-08  9.07328327e-09 -5.07354852e-08\n",
      " -1.21455134e-08  9.64809753e+01 -8.37849347e-09 -2.21845012e-08\n",
      " -1.68431214e-08  8.66473116e+01  8.97239809e+01 -3.27100660e-08\n",
      " -3.43727109e-08  1.64840721e-08  7.15665335e-08  8.88736718e+01\n",
      "  5.54034143e-09  4.26638396e-08  4.97138207e-08 -4.88216029e-08\n",
      "  2.94761519e-08  1.07124692e-07  3.87746251e-09  2.69764391e-08\n",
      " -1.85066043e-08 -9.26593771e-09 -1.63846264e-08  7.33364062e-08\n",
      " -4.57238573e-08  6.75752390e-08  8.91551091e-08 -4.97664897e-08\n",
      "  1.21184123e-08  3.10479911e-08  7.90770301e+01  4.27664434e-08\n",
      " -4.23936544e-08  8.96959543e-09 -1.36740988e-08  6.59502077e+01\n",
      " -3.21201753e-08  8.37007651e-08  5.61675488e-08  4.28115261e-08\n",
      "  6.08523764e-08 -1.81747633e-08  3.83780401e-08 -4.14092664e-08\n",
      "  2.09428964e-08 -7.83171664e-08 -1.15262429e-07  4.36188784e-08\n",
      " -5.61817080e-08  1.62008176e-08  2.65832949e-09 -4.09130817e-09\n",
      "  4.03720197e-08  3.89002712e-08  1.64173538e-08  1.65534572e-08\n",
      "  3.14977316e-09  8.23708159e-08 -2.87666253e-08  1.56474504e-08\n",
      " -5.06685923e-09  7.45959898e+01  3.02675994e-08  2.60684153e-08]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Генерация данных\n",
    "X, y = make_regression(n_samples=10000)\n",
    "\n",
    "# Обучение модели LinearRegression\n",
    "reg_linear = LinearRegression().fit(X, y)\n",
    "mse_linear = mean_squared_error(y, reg_linear.predict(X))\n",
    "print(\"MSE Linear Regression:\", mse_linear)\n",
    "\n",
    "# Обучение модели SGDRegressor с подбором гиперпараметров\n",
    "reg_sgd = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "mse_sgd = mean_squared_error(y, reg_sgd.predict(X))\n",
    "print(\"MSE SGDRegressor:\", mse_sgd)\n",
    "print(\"Coefficients SGDRegressor:\", reg_sgd.coef_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T06:50:31.018435500Z",
     "start_time": "2023-12-13T06:50:26.886929300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:12:14.594882300Z",
     "start_time": "2023-12-13T08:12:14.581615200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000, regularization=None):\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.regularization = regularization\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            y_pred = np.dot(X, self.weights)\n",
    "            error = y_pred - y\n",
    "            gradient = (2 / n_samples) * np.dot(X.T, error)\n",
    "\n",
    "            if self.regularization == 'l1':\n",
    "                gradient += self.l_ratio * np.sign(self.weights)\n",
    "            elif self.regularization == 'l2':\n",
    "                gradient += 2 * self.l_ratio * self.weights\n",
    "\n",
    "            self.weights -= self.alpha * gradient\n",
    "\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            print(f\"Iteration {iteration + 1}, MSE: {mse}\")\n",
    "\n",
    "            if np.linalg.norm(self.weights - gradient) < self.tol:\n",
    "                break\n",
    "\n",
    "        print(f\"Training completed. Final weights: {self.weights}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "# Пример использования\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([3, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T08:12:17.513748600Z",
     "start_time": "2023-12-13T08:12:17.072495900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, MSE: 59.666666666666664\n",
      "Iteration 2, MSE: 58.92754847407408\n",
      "Iteration 3, MSE: 58.197589209168626\n",
      "Iteration 4, MSE: 57.47667537698319\n",
      "Iteration 5, MSE: 56.764694888949776\n",
      "Iteration 6, MSE: 56.06153704547179\n",
      "Iteration 7, MSE: 55.367092518712276\n",
      "Iteration 8, MSE: 54.68125333559547\n",
      "Iteration 9, MSE: 54.0039128610189\n",
      "Iteration 10, MSE: 53.3349657812736\n",
      "Iteration 11, MSE: 52.67430808766988\n",
      "Iteration 12, MSE: 52.02183706036565\n",
      "Iteration 13, MSE: 51.37745125239556\n",
      "Iteration 14, MSE: 50.74105047389778\n",
      "Iteration 15, MSE: 50.112535776536276\n",
      "Iteration 16, MSE: 49.49180943811623\n",
      "Iteration 17, MSE: 48.87877494738998\n",
      "Iteration 18, MSE: 48.27333698905132\n",
      "Iteration 19, MSE: 47.67540142891565\n",
      "Iteration 20, MSE: 47.08487529928394\n",
      "Iteration 21, MSE: 46.50166678448784\n",
      "Iteration 22, MSE: 45.92568520661414\n",
      "Iteration 23, MSE: 45.35684101140587\n",
      "Iteration 24, MSE: 44.79504575433844\n",
      "Iteration 25, MSE: 44.240212086867935\n",
      "Iteration 26, MSE: 43.6922537428502\n",
      "Iteration 27, MSE: 43.151085525127876\n",
      "Iteration 28, MSE: 42.616623292283926\n",
      "Iteration 29, MSE: 42.08878394555912\n",
      "Iteration 30, MSE: 41.567485415931706\n",
      "Iteration 31, MSE: 41.052646651357186\n",
      "Iteration 32, MSE: 40.54418760416622\n",
      "Iteration 33, MSE: 40.04202921861867\n",
      "Iteration 34, MSE: 39.54609341861186\n",
      "Iteration 35, MSE: 39.05630309554122\n",
      "Iteration 36, MSE: 38.57258209631123\n",
      "Iteration 37, MSE: 38.09485521149511\n",
      "Iteration 38, MSE: 37.62304816364105\n",
      "Iteration 39, MSE: 37.157087595723475\n",
      "Iteration 40, MSE: 36.69690105973733\n",
      "Iteration 41, MSE: 36.24241700543377\n",
      "Iteration 42, MSE: 35.7935647691954\n",
      "Iteration 43, MSE: 35.35027456304939\n",
      "Iteration 44, MSE: 34.912477463816685\n",
      "Iteration 45, MSE: 34.48010540239574\n",
      "Iteration 46, MSE: 34.05309115317906\n",
      "Iteration 47, MSE: 33.63136832360075\n",
      "Iteration 48, MSE: 33.21487134381377\n",
      "Iteration 49, MSE: 32.80353545649498\n",
      "Iteration 50, MSE: 32.397296706776615\n",
      "Iteration 51, MSE: 31.996091932302406\n",
      "Iteration 52, MSE: 31.59985875340705\n",
      "Iteration 53, MSE: 31.208535563417268\n",
      "Iteration 54, MSE: 30.822061519073106\n",
      "Iteration 55, MSE: 30.440376531067972\n",
      "Iteration 56, MSE: 30.063421254705762\n",
      "Iteration 57, MSE: 29.691137080673922\n",
      "Iteration 58, MSE: 29.323466125930732\n",
      "Iteration 59, MSE: 28.960351224705565\n",
      "Iteration 60, MSE: 28.60173591961067\n",
      "Iteration 61, MSE: 28.247564452863035\n",
      "Iteration 62, MSE: 27.89778175761516\n",
      "Iteration 63, MSE: 27.55233344939305\n",
      "Iteration 64, MSE: 27.21116581764055\n",
      "Iteration 65, MSE: 26.874225817368252\n",
      "Iteration 66, MSE: 26.541461060905988\n",
      "Iteration 67, MSE: 26.212819809757537\n",
      "Iteration 68, MSE: 25.888250966556203\n",
      "Iteration 69, MSE: 25.567704067120122\n",
      "Iteration 70, MSE: 25.251129272606004\n",
      "Iteration 71, MSE: 24.938477361760118\n",
      "Iteration 72, MSE: 24.629699723265265\n",
      "Iteration 73, MSE: 24.324748348182638\n",
      "Iteration 74, MSE: 24.02357582248727\n",
      "Iteration 75, MSE: 23.726135319696056\n",
      "Iteration 76, MSE: 23.432380593587094\n",
      "Iteration 77, MSE: 23.14226597100921\n",
      "Iteration 78, MSE: 22.85574634478066\n",
      "Iteration 79, MSE: 22.572777166675763\n",
      "Iteration 80, MSE: 22.293314440498467\n",
      "Iteration 81, MSE: 22.017314715241767\n",
      "Iteration 82, MSE: 21.744735078331853\n",
      "Iteration 83, MSE: 21.47553314895599\n",
      "Iteration 84, MSE: 21.20966707147309\n",
      "Iteration 85, MSE: 20.947095508905893\n",
      "Iteration 86, MSE: 20.687777636513886\n",
      "Iteration 87, MSE: 20.431673135445735\n",
      "Iteration 88, MSE: 20.178742186470483\n",
      "Iteration 89, MSE: 19.928945463786395\n",
      "Iteration 90, MSE: 19.682244128906472\n",
      "Iteration 91, MSE: 19.438599824619825\n",
      "Iteration 92, MSE: 19.197974669027797\n",
      "Iteration 93, MSE: 18.960331249654043\n",
      "Iteration 94, MSE: 18.725632617627575\n",
      "Iteration 95, MSE: 18.493842281937862\n",
      "Iteration 96, MSE: 18.264924203761186\n",
      "Iteration 97, MSE: 18.038842790857235\n",
      "Iteration 98, MSE: 17.815562892035164\n",
      "Iteration 99, MSE: 17.595049791688254\n",
      "Iteration 100, MSE: 17.37726920439623\n",
      "Iteration 101, MSE: 17.16218726959455\n",
      "Iteration 102, MSE: 16.94977054630967\n",
      "Iteration 103, MSE: 16.7399860079596\n",
      "Iteration 104, MSE: 16.532801037218892\n",
      "Iteration 105, MSE: 16.32818342094721\n",
      "Iteration 106, MSE: 16.12610134518079\n",
      "Iteration 107, MSE: 15.926523390185972\n",
      "Iteration 108, MSE: 15.729418525573978\n",
      "Iteration 109, MSE: 15.53475610547627\n",
      "Iteration 110, MSE: 15.34250586377967\n",
      "Iteration 111, MSE: 15.152637909420541\n",
      "Iteration 112, MSE: 14.965122721737279\n",
      "Iteration 113, MSE: 14.779931145880361\n",
      "Iteration 114, MSE: 14.597034388279345\n",
      "Iteration 115, MSE: 14.41640401216596\n",
      "Iteration 116, MSE: 14.238011933152722\n",
      "Iteration 117, MSE: 14.06183041486634\n",
      "Iteration 118, MSE: 13.887832064635191\n",
      "Iteration 119, MSE: 13.715989829230283\n",
      "Iteration 120, MSE: 13.546276990658965\n",
      "Iteration 121, MSE: 13.378667162010755\n",
      "Iteration 122, MSE: 13.213134283354693\n",
      "Iteration 123, MSE: 13.049652617687459\n",
      "Iteration 124, MSE: 12.888196746931776\n",
      "Iteration 125, MSE: 12.728741567984335\n",
      "Iteration 126, MSE: 12.571262288812733\n",
      "Iteration 127, MSE: 12.415734424600783\n",
      "Iteration 128, MSE: 12.262133793941544\n",
      "Iteration 129, MSE: 12.110436515077579\n",
      "Iteration 130, MSE: 11.960619002187755\n",
      "Iteration 131, MSE: 11.812657961720108\n",
      "Iteration 132, MSE: 11.666530388770108\n",
      "Iteration 133, MSE: 11.522213563503803\n",
      "Iteration 134, MSE: 11.379685047625328\n",
      "Iteration 135, MSE: 11.23892268088815\n",
      "Iteration 136, MSE: 11.099904577649545\n",
      "Iteration 137, MSE: 10.962609123467791\n",
      "Iteration 138, MSE: 10.827014971741528\n",
      "Iteration 139, MSE: 10.693101040390728\n",
      "Iteration 140, MSE: 10.56084650857884\n",
      "Iteration 141, MSE: 10.430230813475518\n",
      "Iteration 142, MSE: 10.301233647059483\n",
      "Iteration 143, MSE: 10.173834952960997\n",
      "Iteration 144, MSE: 10.04801492334346\n",
      "Iteration 145, MSE: 9.923753995823667\n",
      "Iteration 146, MSE: 9.801032850430198\n",
      "Iteration 147, MSE: 9.679832406599527\n",
      "Iteration 148, MSE: 9.560133820209346\n",
      "Iteration 149, MSE: 9.44191848064864\n",
      "Iteration 150, MSE: 9.32516800792409\n",
      "Iteration 151, MSE: 9.209864249802296\n",
      "Iteration 152, MSE: 9.095989278987478\n",
      "Iteration 153, MSE: 8.983525390334066\n",
      "Iteration 154, MSE: 8.872455098093905\n",
      "Iteration 155, MSE: 8.762761133197527\n",
      "Iteration 156, MSE: 8.65442644056913\n",
      "Iteration 157, MSE: 8.547434176474827\n",
      "Iteration 158, MSE: 8.44176770590375\n",
      "Iteration 159, MSE: 8.337410599981622\n",
      "Iteration 160, MSE: 8.23434663341635\n",
      "Iteration 161, MSE: 8.132559781975308\n",
      "Iteration 162, MSE: 8.032034219993845\n",
      "Iteration 163, MSE: 7.932754317914694\n",
      "Iteration 164, MSE: 7.834704639857861\n",
      "Iteration 165, MSE: 7.737869941220623\n",
      "Iteration 166, MSE: 7.6422351663072865\n",
      "Iteration 167, MSE: 7.547785445988283\n",
      "Iteration 168, MSE: 7.454506095388296\n",
      "Iteration 169, MSE: 7.362382611603048\n",
      "Iteration 170, MSE: 7.271400671444344\n",
      "Iteration 171, MSE: 7.181546129213088\n",
      "Iteration 172, MSE: 7.092805014499891\n",
      "Iteration 173, MSE: 7.005163530012905\n",
      "Iteration 174, MSE: 6.918608049432632\n",
      "Iteration 175, MSE: 6.83312511529325\n",
      "Iteration 176, MSE: 6.748701436890241\n",
      "Iteration 177, MSE: 6.66532388821392\n",
      "Iteration 178, MSE: 6.582979505908592\n",
      "Iteration 179, MSE: 6.501655487256975\n",
      "Iteration 180, MSE: 6.421339188189617\n",
      "Iteration 181, MSE: 6.342018121318982\n",
      "Iteration 182, MSE: 6.26367995399788\n",
      "Iteration 183, MSE: 6.18631250640196\n",
      "Iteration 184, MSE: 6.109903749636\n",
      "Iteration 185, MSE: 6.03444180386359\n",
      "Iteration 186, MSE: 5.959914936460081\n",
      "Iteration 187, MSE: 5.886311560188343\n",
      "Iteration 188, MSE: 5.813620231397173\n",
      "Iteration 189, MSE: 5.74182964824202\n",
      "Iteration 190, MSE: 5.67092864892775\n",
      "Iteration 191, MSE: 5.600906209973199\n",
      "Iteration 192, MSE: 5.5317514444972105\n",
      "Iteration 193, MSE: 5.46345360052594\n",
      "Iteration 194, MSE: 5.396002059321103\n",
      "Iteration 195, MSE: 5.329386333728976\n",
      "Iteration 196, MSE: 5.263596066549815\n",
      "Iteration 197, MSE: 5.198621028927518\n",
      "Iteration 198, MSE: 5.134451118759215\n",
      "Iteration 199, MSE: 5.071076359124579\n",
      "Iteration 200, MSE: 5.008486896734593\n",
      "Iteration 201, MSE: 4.946673000399561\n",
      "Iteration 202, MSE: 4.885625059516056\n",
      "Iteration 203, MSE: 4.825333582572685\n",
      "Iteration 204, MSE: 4.765789195674295\n",
      "Iteration 205, MSE: 4.706982641084529\n",
      "Iteration 206, MSE: 4.64890477578641\n",
      "Iteration 207, MSE: 4.591546570060772\n",
      "Iteration 208, MSE: 4.534899106082292\n",
      "Iteration 209, MSE: 4.478953576532953\n",
      "Iteration 210, MSE: 4.423701283232646\n",
      "Iteration 211, MSE: 4.369133635786772\n",
      "Iteration 212, MSE: 4.315242150250598\n",
      "Iteration 213, MSE: 4.262018447810151\n",
      "Iteration 214, MSE: 4.2094542534794614\n",
      "Iteration 215, MSE: 4.157541394813972\n",
      "Iteration 216, MSE: 4.106271800639855\n",
      "Iteration 217, MSE: 4.055637499799101\n",
      "Iteration 218, MSE: 4.005630619910145\n",
      "Iteration 219, MSE: 3.9562433861438584\n",
      "Iteration 220, MSE: 3.9074681200147023\n",
      "Iteration 221, MSE: 3.8592972381868798\n",
      "Iteration 222, MSE: 3.811723251295233\n",
      "Iteration 223, MSE: 3.7647387627808144\n",
      "Iteration 224, MSE: 3.718336467740849\n",
      "Iteration 225, MSE: 3.6725091517929456\n",
      "Iteration 226, MSE: 3.6272496899534024\n",
      "Iteration 227, MSE: 3.582551045529405\n",
      "Iteration 228, MSE: 3.5384062690249363\n",
      "Iteration 229, MSE: 3.494808497060268\n",
      "Iteration 230, MSE: 3.451750951304837\n",
      "Iteration 231, MSE: 3.4092269374233113\n",
      "Iteration 232, MSE: 3.367229844034784\n",
      "Iteration 233, MSE: 3.325753141684786\n",
      "Iteration 234, MSE: 3.284790381830103\n",
      "Iteration 235, MSE: 3.244335195836134\n",
      "Iteration 236, MSE: 3.2043812939866734\n",
      "Iteration 237, MSE: 3.164922464506004\n",
      "Iteration 238, MSE: 3.125952572593057\n",
      "Iteration 239, MSE: 3.087465559467562\n",
      "Iteration 240, MSE: 3.0494554414280315\n",
      "Iteration 241, MSE: 3.011916308921402\n",
      "Iteration 242, MSE: 2.974842325624193\n",
      "Iteration 243, MSE: 2.9382277275350863\n",
      "Iteration 244, MSE: 2.9020668220787083\n",
      "Iteration 245, MSE: 2.8663539872205512\n",
      "Iteration 246, MSE: 2.831083670592854\n",
      "Iteration 247, MSE: 2.796250388631297\n",
      "Iteration 248, MSE: 2.761848725722413\n",
      "Iteration 249, MSE: 2.7278733333615715\n",
      "Iteration 250, MSE: 2.6943189293213625\n",
      "Iteration 251, MSE: 2.6611802968303278\n",
      "Iteration 252, MSE: 2.6284522837618294\n",
      "Iteration 253, MSE: 2.596129801833001\n",
      "Iteration 254, MSE: 2.5642078258136087\n",
      "Iteration 255, MSE: 2.5326813927447236\n",
      "Iteration 256, MSE: 2.5015456011670687\n",
      "Iteration 257, MSE: 2.4707956103589264\n",
      "Iteration 258, MSE: 2.4404266395835097\n",
      "Iteration 259, MSE: 2.4104339673456283\n",
      "Iteration 260, MSE: 2.3808129306575974\n",
      "Iteration 261, MSE: 2.351558924314219\n",
      "Iteration 262, MSE: 2.32266740017676\n",
      "Iteration 263, MSE: 2.2941338664658013\n",
      "Iteration 264, MSE: 2.2659538870628424\n",
      "Iteration 265, MSE: 2.2381230808205737\n",
      "Iteration 266, MSE: 2.210637120881682\n",
      "Iteration 267, MSE: 2.1834917340061164\n",
      "Iteration 268, MSE: 2.156682699906653\n",
      "Iteration 269, MSE: 2.1302058505927435\n",
      "Iteration 270, MSE: 2.1040570697224528\n",
      "Iteration 271, MSE: 2.078232291962447\n",
      "Iteration 272, MSE: 2.0527275023559173\n",
      "Iteration 273, MSE: 2.0275387356983114\n",
      "Iteration 274, MSE: 2.0026620759208273\n",
      "Iteration 275, MSE: 1.9780936554815405\n",
      "Iteration 276, MSE: 1.9538296547640623\n",
      "Iteration 277, MSE: 1.9298663014836601\n",
      "Iteration 278, MSE: 1.906199870100745\n",
      "Iteration 279, MSE: 1.8828266812416075\n",
      "Iteration 280, MSE: 1.8597431011263528\n",
      "Iteration 281, MSE: 1.836945541003906\n",
      "Iteration 282, MSE: 1.8144304565940352\n",
      "Iteration 283, MSE: 1.7921943475362656\n",
      "Iteration 284, MSE: 1.7702337568456594\n",
      "Iteration 285, MSE: 1.7485452703753035\n",
      "Iteration 286, MSE: 1.7271255162854733\n",
      "Iteration 287, MSE: 1.7059711645193758\n",
      "Iteration 288, MSE: 1.685078926285387\n",
      "Iteration 289, MSE: 1.6644455535457023\n",
      "Iteration 290, MSE: 1.644067838511333\n",
      "Iteration 291, MSE: 1.6239426131433448\n",
      "Iteration 292, MSE: 1.604066748660293\n",
      "Iteration 293, MSE: 1.584437155051743\n",
      "Iteration 294, MSE: 1.5650507805978522\n",
      "Iteration 295, MSE: 1.5459046113948594\n",
      "Iteration 296, MSE: 1.5269956708864936\n",
      "Iteration 297, MSE: 1.5083210194011645\n",
      "Iteration 298, MSE: 1.489877753694903\n",
      "Iteration 299, MSE: 1.4716630064999592\n",
      "Iteration 300, MSE: 1.4536739460789938\n",
      "Iteration 301, MSE: 1.4359077757847991\n",
      "Iteration 302, MSE: 1.4183617336254624\n",
      "Iteration 303, MSE: 1.4010330918349292\n",
      "Iteration 304, MSE: 1.3839191564488986\n",
      "Iteration 305, MSE: 1.3670172668859413\n",
      "Iteration 306, MSE: 1.3503247955338435\n",
      "Iteration 307, MSE: 1.3338391473410518\n",
      "Iteration 308, MSE: 1.3175577594131933\n",
      "Iteration 309, MSE: 1.3014781006146\n",
      "Iteration 310, MSE: 1.285597671174748\n",
      "Iteration 311, MSE: 1.2699140022996003\n",
      "Iteration 312, MSE: 1.2544246557877496\n",
      "Iteration 313, MSE: 1.239127223651334\n",
      "Iteration 314, MSE: 1.2240193277416185\n",
      "Iteration 315, MSE: 1.2090986193792508\n",
      "Iteration 316, MSE: 1.1943627789890832\n",
      "Iteration 317, MSE: 1.1798095157395223\n",
      "Iteration 318, MSE: 1.1654365671863378\n",
      "Iteration 319, MSE: 1.1512416989209016\n",
      "Iteration 320, MSE: 1.1372227042227785\n",
      "Iteration 321, MSE: 1.1233774037166153\n",
      "Iteration 322, MSE: 1.1097036450332995\n",
      "Iteration 323, MSE: 1.096199302475289\n",
      "Iteration 324, MSE: 1.0828622766861205\n",
      "Iteration 325, MSE: 1.0696904943239893\n",
      "Iteration 326, MSE: 1.0566819077393823\n",
      "Iteration 327, MSE: 1.0438344946567124\n",
      "Iteration 328, MSE: 1.0311462578598836\n",
      "Iteration 329, MSE: 1.018615224881769\n",
      "Iteration 330, MSE: 1.006239447697522\n",
      "Iteration 331, MSE: 0.9940170024216942\n",
      "Iteration 332, MSE: 0.9819459890091097\n",
      "Iteration 333, MSE: 0.9700245309594434\n",
      "Iteration 334, MSE: 0.9582507750254533\n",
      "Iteration 335, MSE: 0.9466228909248416\n",
      "Iteration 336, MSE: 0.9351390710556747\n",
      "Iteration 337, MSE: 0.923797530215341\n",
      "Iteration 338, MSE: 0.9125965053229703\n",
      "Iteration 339, MSE: 0.9015342551453255\n",
      "Iteration 340, MSE: 0.8906090600260571\n",
      "Iteration 341, MSE: 0.8798192216183396\n",
      "Iteration 342, MSE: 0.8691630626207978\n",
      "Iteration 343, MSE: 0.8586389265167335\n",
      "Iteration 344, MSE: 0.8482451773165499\n",
      "Iteration 345, MSE: 0.8379801993033933\n",
      "Iteration 346, MSE: 0.8278423967819446\n",
      "Iteration 347, MSE: 0.8178301938303075\n",
      "Iteration 348, MSE: 0.8079420340549861\n",
      "Iteration 349, MSE: 0.7981763803488975\n",
      "Iteration 350, MSE: 0.7885317146523708\n",
      "Iteration 351, MSE: 0.7790065377171261\n",
      "Iteration 352, MSE: 0.769599368873162\n",
      "Iteration 353, MSE: 0.7603087457985462\n",
      "Iteration 354, MSE: 0.7511332242920424\n",
      "Iteration 355, MSE: 0.7420713780485686\n",
      "Iteration 356, MSE: 0.7331217984374305\n",
      "Iteration 357, MSE: 0.7242830942833097\n",
      "Iteration 358, MSE: 0.7155538916499545\n",
      "Iteration 359, MSE: 0.7069328336265629\n",
      "Iteration 360, MSE: 0.6984185801168087\n",
      "Iteration 361, MSE: 0.690009807630477\n",
      "Iteration 362, MSE: 0.6817052090776842\n",
      "Iteration 363, MSE: 0.6735034935656605\n",
      "Iteration 364, MSE: 0.6654033861980241\n",
      "Iteration 365, MSE: 0.6574036278765728\n",
      "Iteration 366, MSE: 0.6495029751055074\n",
      "Iteration 367, MSE: 0.6417001997980915\n",
      "Iteration 368, MSE: 0.6339940890857146\n",
      "Iteration 369, MSE: 0.626383445129304\n",
      "Iteration 370, MSE: 0.6188670849330893\n",
      "Iteration 371, MSE: 0.6114438401606622\n",
      "Iteration 372, MSE: 0.6041125569533262\n",
      "Iteration 373, MSE: 0.5968720957506931\n",
      "Iteration 374, MSE: 0.5897213311134982\n",
      "Iteration 375, MSE: 0.582659151548615\n",
      "Iteration 376, MSE: 0.5756844593362487\n",
      "Iteration 377, MSE: 0.5687961703592442\n",
      "Iteration 378, MSE: 0.5619932139345397\n",
      "Iteration 379, MSE: 0.5552745326466895\n",
      "Iteration 380, MSE: 0.5486390821834515\n",
      "Iteration 381, MSE: 0.5420858311734252\n",
      "Iteration 382, MSE: 0.5356137610256821\n",
      "Iteration 383, MSE: 0.5292218657714047\n",
      "Iteration 384, MSE: 0.5229091519074626\n",
      "Iteration 385, MSE: 0.5166746382419553\n",
      "Iteration 386, MSE: 0.5105173557416453\n",
      "Iteration 387, MSE: 0.504436347381296\n",
      "Iteration 388, MSE: 0.4984306679948666\n",
      "Iteration 389, MSE: 0.4924993841285617\n",
      "Iteration 390, MSE: 0.4866415738956946\n",
      "Iteration 391, MSE: 0.4808563268333521\n",
      "Iteration 392, MSE: 0.47514274376082727\n",
      "Iteration 393, MSE: 0.4694999366398221\n",
      "Iteration 394, MSE: 0.46392702843636985\n",
      "Iteration 395, MSE: 0.4584231529844729\n",
      "Iteration 396, MSE: 0.45298745485142783\n",
      "Iteration 397, MSE: 0.4476190892048248\n",
      "Iteration 398, MSE: 0.4423172216811894\n",
      "Iteration 399, MSE: 0.4370810282562541\n",
      "Iteration 400, MSE: 0.4319096951168402\n",
      "Iteration 401, MSE: 0.4268024185343196\n",
      "Iteration 402, MSE: 0.42175840473965653\n",
      "Iteration 403, MSE: 0.41677686979998413\n",
      "Iteration 404, MSE: 0.41185703949672475\n",
      "Iteration 405, MSE: 0.4069981492052057\n",
      "Iteration 406, MSE: 0.40219944377577804\n",
      "Iteration 407, MSE: 0.39746017741640455\n",
      "Iteration 408, MSE: 0.39277961357670144\n",
      "Iteration 409, MSE: 0.38815702483342\n",
      "Iteration 410, MSE: 0.383591692777345\n",
      "Iteration 411, MSE: 0.3790829079015934\n",
      "Iteration 412, MSE: 0.3746299694912991\n",
      "Iteration 413, MSE: 0.37023218551467063\n",
      "Iteration 414, MSE: 0.36588887251538527\n",
      "Iteration 415, MSE: 0.3615993555063249\n",
      "Iteration 416, MSE: 0.35736296786463423\n",
      "Iteration 417, MSE: 0.35317905122806675\n",
      "Iteration 418, MSE: 0.34904695539262404\n",
      "Iteration 419, MSE: 0.34496603821146116\n",
      "Iteration 420, MSE: 0.34093566549504195\n",
      "Iteration 421, MSE: 0.3369552109125388\n",
      "Iteration 422, MSE: 0.33302405589444645\n",
      "Iteration 423, MSE: 0.3291415895364038\n",
      "Iteration 424, MSE: 0.32530720850421735\n",
      "Iteration 425, MSE: 0.3215203169400425\n",
      "Iteration 426, MSE: 0.3177803263697479\n",
      "Iteration 427, MSE: 0.3140866556114154\n",
      "Iteration 428, MSE: 0.310438730684976\n",
      "Iteration 429, MSE: 0.30683598472296714\n",
      "Iteration 430, MSE: 0.3032778578823952\n",
      "Iteration 431, MSE: 0.2997637972576902\n",
      "Iteration 432, MSE: 0.29629325679473906\n",
      "Iteration 433, MSE: 0.2928656972059837\n",
      "Iteration 434, MSE: 0.28948058588657416\n",
      "Iteration 435, MSE: 0.2861373968315526\n",
      "Iteration 436, MSE: 0.28283561055407797\n",
      "Iteration 437, MSE: 0.2795747140046449\n",
      "Iteration 438, MSE: 0.2763542004913239\n",
      "Iteration 439, MSE: 0.27317356960097366\n",
      "Iteration 440, MSE: 0.2700323271214355\n",
      "Iteration 441, MSE: 0.2669299849646991\n",
      "Iteration 442, MSE: 0.2638660610910047\n",
      "Iteration 443, MSE: 0.26084007943390325\n",
      "Iteration 444, MSE: 0.2578515698262308\n",
      "Iteration 445, MSE: 0.25490006792701\n",
      "Iteration 446, MSE: 0.2519851151492476\n",
      "Iteration 447, MSE: 0.24910625858864188\n",
      "Iteration 448, MSE: 0.24626305095315634\n",
      "Iteration 449, MSE: 0.24345505049347804\n",
      "Iteration 450, MSE: 0.2406818209343322\n",
      "Iteration 451, MSE: 0.23794293140664916\n",
      "Iteration 452, MSE: 0.23523795638057318\n",
      "Iteration 453, MSE: 0.23256647559930033\n",
      "Iteration 454, MSE: 0.2299280740137337\n",
      "Iteration 455, MSE: 0.2273223417179556\n",
      "Iteration 456, MSE: 0.2247488738854909\n",
      "Iteration 457, MSE: 0.222207270706365\n",
      "Iteration 458, MSE: 0.2196971373249398\n",
      "Iteration 459, MSE: 0.21721808377852325\n",
      "Iteration 460, MSE: 0.21476972493673663\n",
      "Iteration 461, MSE: 0.2123516804416319\n",
      "Iteration 462, MSE: 0.2099635746485522\n",
      "Iteration 463, MSE: 0.20760503656773177\n",
      "Iteration 464, MSE: 0.2052756998066084\n",
      "Iteration 465, MSE: 0.20297520251285606\n",
      "Iteration 466, MSE: 0.20070318731812894\n",
      "Iteration 467, MSE: 0.19845930128248926\n",
      "Iteration 468, MSE: 0.19624319583953798\n",
      "Iteration 469, MSE: 0.19405452674221865\n",
      "Iteration 470, MSE: 0.1918929540092884\n",
      "Iteration 471, MSE: 0.18975814187246162\n",
      "Iteration 472, MSE: 0.18764975872420256\n",
      "Iteration 473, MSE: 0.18556747706616775\n",
      "Iteration 474, MSE: 0.18351097345828252\n",
      "Iteration 475, MSE: 0.18147992846845654\n",
      "Iteration 476, MSE: 0.1794740266229153\n",
      "Iteration 477, MSE: 0.17749295635714923\n",
      "Iteration 478, MSE: 0.17553640996747252\n",
      "Iteration 479, MSE: 0.173604083563183\n",
      "Iteration 480, MSE: 0.1716956770193051\n",
      "Iteration 481, MSE: 0.16981089392993662\n",
      "Iteration 482, MSE: 0.16794944156215252\n",
      "Iteration 483, MSE: 0.16611103081049758\n",
      "Iteration 484, MSE: 0.16429537615203407\n",
      "Iteration 485, MSE: 0.16250219560194587\n",
      "Iteration 486, MSE: 0.16073121066969717\n",
      "Iteration 487, MSE: 0.1589821463157324\n",
      "Iteration 488, MSE: 0.15725473090871236\n",
      "Iteration 489, MSE: 0.15554869618327616\n",
      "Iteration 490, MSE: 0.1538637771983399\n",
      "Iteration 491, MSE: 0.1521997122958963\n",
      "Iteration 492, MSE: 0.15055624306033286\n",
      "Iteration 493, MSE: 0.14893311427825465\n",
      "Iteration 494, MSE: 0.14733007389880118\n",
      "Iteration 495, MSE: 0.14574687299445727\n",
      "Iteration 496, MSE: 0.14418326572235493\n",
      "Iteration 497, MSE: 0.14263900928603865\n",
      "Iteration 498, MSE: 0.14111386389772498\n",
      "Iteration 499, MSE: 0.13960759274101517\n",
      "Iteration 500, MSE: 0.13811996193407414\n",
      "Iteration 501, MSE: 0.13665074049326986\n",
      "Iteration 502, MSE: 0.13519970029725417\n",
      "Iteration 503, MSE: 0.1337666160515\n",
      "Iteration 504, MSE: 0.13235126525326643\n",
      "Iteration 505, MSE: 0.1309534281570073\n",
      "Iteration 506, MSE: 0.12957288774020487\n",
      "Iteration 507, MSE: 0.1282094296696252\n",
      "Iteration 508, MSE: 0.12686284226799402\n",
      "Iteration 509, MSE: 0.12553291648108472\n",
      "Iteration 510, MSE: 0.12421944584521355\n",
      "Iteration 511, MSE: 0.12292222645514024\n",
      "Iteration 512, MSE: 0.12164105693236194\n",
      "Iteration 513, MSE: 0.12037573839380471\n",
      "Iteration 514, MSE: 0.11912607442089936\n",
      "Iteration 515, MSE: 0.11789187102904318\n",
      "Iteration 516, MSE: 0.11667293663743743\n",
      "Iteration 517, MSE: 0.11546908203930063\n",
      "Iteration 518, MSE: 0.11428012037245183\n",
      "Iteration 519, MSE: 0.11310586709025544\n",
      "Iteration 520, MSE: 0.11194613993292746\n",
      "Iteration 521, MSE: 0.11080075889919765\n",
      "Iteration 522, MSE: 0.10966954621832321\n",
      "Iteration 523, MSE: 0.10855232632244786\n",
      "Iteration 524, MSE: 0.10744892581930639\n",
      "Iteration 525, MSE: 0.10635917346526157\n",
      "Iteration 526, MSE: 0.10528290013868309\n",
      "Iteration 527, MSE: 0.10421993881364801\n",
      "Iteration 528, MSE: 0.10317012453397445\n",
      "Iteration 529, MSE: 0.10213329438757246\n",
      "Iteration 530, MSE: 0.10110928748111675\n",
      "Iteration 531, MSE: 0.10009794491502562\n",
      "Iteration 532, MSE: 0.09909910975875996\n",
      "Iteration 533, MSE: 0.0981126270264206\n",
      "Iteration 534, MSE: 0.09713834365265166\n",
      "Iteration 535, MSE: 0.09617610846884267\n",
      "Iteration 536, MSE: 0.09522577217962293\n",
      "Iteration 537, MSE: 0.09428718733964973\n",
      "Iteration 538, MSE: 0.09336020833068277\n",
      "Iteration 539, MSE: 0.09244469133894428\n",
      "Iteration 540, MSE: 0.09154049433275667\n",
      "Iteration 541, MSE: 0.09064747704046294\n",
      "Iteration 542, MSE: 0.08976550092861169\n",
      "Iteration 543, MSE: 0.08889442918042202\n",
      "Iteration 544, MSE: 0.08803412667450962\n",
      "Iteration 545, MSE: 0.08718445996387657\n",
      "Iteration 546, MSE: 0.08634529725516578\n",
      "Iteration 547, MSE: 0.08551650838816688\n",
      "Iteration 548, MSE: 0.08469796481557844\n",
      "Iteration 549, MSE: 0.08388953958302413\n",
      "Iteration 550, MSE: 0.08309110730931148\n",
      "Iteration 551, MSE: 0.08230254416693648\n",
      "Iteration 552, MSE: 0.08152372786283478\n",
      "Iteration 553, MSE: 0.08075453761936054\n",
      "Iteration 554, MSE: 0.07999485415551612\n",
      "Iteration 555, MSE: 0.07924455966839918\n",
      "Iteration 556, MSE: 0.07850353781488811\n",
      "Iteration 557, MSE: 0.0777716736935562\n",
      "Iteration 558, MSE: 0.07704885382680203\n",
      "Iteration 559, MSE: 0.07633496614321006\n",
      "Iteration 560, MSE: 0.07562989996012026\n",
      "Iteration 561, MSE: 0.07493354596642521\n",
      "Iteration 562, MSE: 0.07424579620556916\n",
      "Iteration 563, MSE: 0.07356654405876553\n",
      "Iteration 564, MSE: 0.07289568422841837\n",
      "Iteration 565, MSE: 0.0722331127217511\n",
      "Iteration 566, MSE: 0.07157872683463735\n",
      "Iteration 567, MSE: 0.07093242513563343\n",
      "Iteration 568, MSE: 0.07029410745020465\n",
      "Iteration 569, MSE: 0.06966367484515347\n",
      "Iteration 570, MSE: 0.06904102961323487\n",
      "Iteration 571, MSE: 0.06842607525796644\n",
      "Iteration 572, MSE: 0.06781871647862239\n",
      "Iteration 573, MSE: 0.06721885915541921\n",
      "Iteration 574, MSE: 0.06662641033487976\n",
      "Iteration 575, MSE: 0.06604127821538018\n",
      "Iteration 576, MSE: 0.0654633721328794\n",
      "Iteration 577, MSE: 0.06489260254681793\n",
      "Iteration 578, MSE: 0.06432888102620116\n",
      "Iteration 579, MSE: 0.0637721202358454\n",
      "Iteration 580, MSE: 0.06322223392280128\n",
      "Iteration 581, MSE: 0.06267913690294308\n",
      "Iteration 582, MSE: 0.06214274504772296\n",
      "Iteration 583, MSE: 0.06161297527109183\n",
      "Iteration 584, MSE: 0.06108974551657995\n",
      "Iteration 585, MSE: 0.06057297474453918\n",
      "Iteration 586, MSE: 0.06006258291954192\n",
      "Iteration 587, MSE: 0.059558490997938966\n",
      "Iteration 588, MSE: 0.05906062091556774\n",
      "Iteration 589, MSE: 0.05856889557561579\n",
      "Iteration 590, MSE: 0.05808323883663304\n",
      "Iteration 591, MSE: 0.05760357550069437\n",
      "Iteration 592, MSE: 0.05712983130170581\n",
      "Iteration 593, MSE: 0.056661932893859464\n",
      "Iteration 594, MSE: 0.05619980784022784\n",
      "Iteration 595, MSE: 0.05574338460150343\n",
      "Iteration 596, MSE: 0.0552925925248727\n",
      "Iteration 597, MSE: 0.054847361833033854\n",
      "Iteration 598, MSE: 0.05440762361334588\n",
      "Iteration 599, MSE: 0.05397330980711474\n",
      "Iteration 600, MSE: 0.05354435319900994\n",
      "Iteration 601, MSE: 0.05312068740661712\n",
      "Iteration 602, MSE: 0.052702246870112136\n",
      "Iteration 603, MSE: 0.052288966842073126\n",
      "Iteration 604, MSE: 0.051880783377408875\n",
      "Iteration 605, MSE: 0.05147763332341784\n",
      "Iteration 606, MSE: 0.05107945430997112\n",
      "Iteration 607, MSE: 0.05068618473981174\n",
      "Iteration 608, MSE: 0.05029776377898119\n",
      "Iteration 609, MSE: 0.04991413134735625\n",
      "Iteration 610, MSE: 0.04953522810931226\n",
      "Iteration 611, MSE: 0.049160995464494206\n",
      "Iteration 612, MSE: 0.04879137553870736\n",
      "Iteration 613, MSE: 0.04842631117491756\n",
      "Iteration 614, MSE: 0.048065745924365945\n",
      "Iteration 615, MSE: 0.04770962403779017\n",
      "Iteration 616, MSE: 0.047357890456757486\n",
      "Iteration 617, MSE: 0.047010490805105574\n",
      "Iteration 618, MSE: 0.04666737138048477\n",
      "Iteration 619, MSE: 0.04632847914601209\n",
      "Iteration 620, MSE: 0.045993761722021485\n",
      "Iteration 621, MSE: 0.04566316737792234\n",
      "Iteration 622, MSE: 0.04533664502415504\n",
      "Iteration 623, MSE: 0.04501414420424751\n",
      "Iteration 624, MSE: 0.04469561508697164\n",
      "Iteration 625, MSE: 0.0443810084585921\n",
      "Iteration 626, MSE: 0.04407027571521901\n",
      "Iteration 627, MSE: 0.043763368855247266\n",
      "Iteration 628, MSE: 0.0434602404718942\n",
      "Iteration 629, MSE: 0.04316084374582992\n",
      "Iteration 630, MSE: 0.042865132437896476\n",
      "Iteration 631, MSE: 0.042573060881919034\n",
      "Iteration 632, MSE: 0.04228458397760488\n",
      "Iteration 633, MSE: 0.04199965718353221\n",
      "Iteration 634, MSE: 0.04171823651022357\n",
      "Iteration 635, MSE: 0.04144027851330844\n",
      "Iteration 636, MSE: 0.041165740286766526\n",
      "Iteration 637, MSE: 0.04089457945625567\n",
      "Iteration 638, MSE: 0.04062675417252682\n",
      "Iteration 639, MSE: 0.0403622231049147\n",
      "Iteration 640, MSE: 0.04010094543491139\n",
      "Iteration 641, MSE: 0.03984288084982226\n",
      "Iteration 642, MSE: 0.03958798953649464\n",
      "Iteration 643, MSE: 0.03933623217513154\n",
      "Iteration 644, MSE: 0.03908756993317417\n",
      "Iteration 645, MSE: 0.03884196445926722\n",
      "Iteration 646, MSE: 0.038599377877294776\n",
      "Iteration 647, MSE: 0.03835977278049074\n",
      "Iteration 648, MSE: 0.03812311222562397\n",
      "Iteration 649, MSE: 0.037889359727252825\n",
      "Iteration 650, MSE: 0.037658479252054504\n",
      "Iteration 651, MSE: 0.03743043521321951\n",
      "Iteration 652, MSE: 0.03720519246492202\n",
      "Iteration 653, MSE: 0.03698271629685237\n",
      "Iteration 654, MSE: 0.03676297242882149\n",
      "Iteration 655, MSE: 0.036545927005431494\n",
      "Iteration 656, MSE: 0.03633154659081149\n",
      "Iteration 657, MSE: 0.03611979816341692\n",
      "Iteration 658, MSE: 0.03591064911089959\n",
      "Iteration 659, MSE: 0.03570406722503241\n",
      "Iteration 660, MSE: 0.03550002069670482\n",
      "Iteration 661, MSE: 0.03529847811097537\n",
      "Iteration 662, MSE: 0.03509940844218857\n",
      "Iteration 663, MSE: 0.03490278104914968\n",
      "Iteration 664, MSE: 0.03470856567036073\n",
      "Iteration 665, MSE: 0.034516732419316756\n",
      "Iteration 666, MSE: 0.034327251779858216\n",
      "Iteration 667, MSE: 0.034140094601580497\n",
      "Iteration 668, MSE: 0.033955232095303586\n",
      "Iteration 669, MSE: 0.03377263582859418\n",
      "Iteration 670, MSE: 0.03359227772134731\n",
      "Iteration 671, MSE: 0.03341413004141689\n",
      "Iteration 672, MSE: 0.0332381654003076\n",
      "Iteration 673, MSE: 0.03306435674891379\n",
      "Iteration 674, MSE: 0.03289267737331459\n",
      "Iteration 675, MSE: 0.032723100890621186\n",
      "Iteration 676, MSE: 0.03255560124487434\n",
      "Iteration 677, MSE: 0.032390152702992656\n",
      "Iteration 678, MSE: 0.03222672985077104\n",
      "Iteration 679, MSE: 0.03206530758893209\n",
      "Iteration 680, MSE: 0.03190586112921964\n",
      "Iteration 681, MSE: 0.031748365990547804\n",
      "Iteration 682, MSE: 0.03159279799519383\n",
      "Iteration 683, MSE: 0.0314391332650375\n",
      "Iteration 684, MSE: 0.03128734821784956\n",
      "Iteration 685, MSE: 0.031137419563626544\n",
      "Iteration 686, MSE: 0.03098932430096656\n",
      "Iteration 687, MSE: 0.030843039713495987\n",
      "Iteration 688, MSE: 0.030698543366334773\n",
      "Iteration 689, MSE: 0.030555813102610647\n",
      "Iteration 690, MSE: 0.03041482704001312\n",
      "Iteration 691, MSE: 0.030275563567390338\n",
      "Iteration 692, MSE: 0.030138001341390542\n",
      "Iteration 693, MSE: 0.030002119283142243\n",
      "Iteration 694, MSE: 0.029867896574977588\n",
      "Iteration 695, MSE: 0.029735312657195895\n",
      "Iteration 696, MSE: 0.029604347224865896\n",
      "Iteration 697, MSE: 0.029474980224669323\n",
      "Iteration 698, MSE: 0.02934719185178332\n",
      "Iteration 699, MSE: 0.029220962546800417\n",
      "Iteration 700, MSE: 0.029096272992688788\n",
      "Iteration 701, MSE: 0.028973104111786076\n",
      "Iteration 702, MSE: 0.028851437062836452\n",
      "Iteration 703, MSE: 0.028731253238058915\n",
      "Iteration 704, MSE: 0.028612534260254795\n",
      "Iteration 705, MSE: 0.02849526197995104\n",
      "Iteration 706, MSE: 0.02837941847257727\n",
      "Iteration 707, MSE: 0.028264986035678966\n",
      "Iteration 708, MSE: 0.028151947186166416\n",
      "Iteration 709, MSE: 0.028040284657594224\n",
      "Iteration 710, MSE: 0.027929981397478848\n",
      "Iteration 711, MSE: 0.02782102056464554\n",
      "Iteration 712, MSE: 0.027713385526611525\n",
      "Iteration 713, MSE: 0.027607059856997948\n",
      "Iteration 714, MSE: 0.02750202733297745\n",
      "Iteration 715, MSE: 0.02739827193275172\n",
      "Iteration 716, MSE: 0.027295777833059306\n",
      "Iteration 717, MSE: 0.027194529406716534\n",
      "Iteration 718, MSE: 0.02709451122018737\n",
      "Iteration 719, MSE: 0.026995708031183185\n",
      "Iteration 720, MSE: 0.02689810478629447\n",
      "Iteration 721, MSE: 0.026801686618648318\n",
      "Iteration 722, MSE: 0.02670643884559812\n",
      "Iteration 723, MSE: 0.0266123469664402\n",
      "Iteration 724, MSE: 0.026519396660160486\n",
      "Iteration 725, MSE: 0.02642757378320562\n",
      "Iteration 726, MSE: 0.026336864367285082\n",
      "Iteration 727, MSE: 0.026247254617200417\n",
      "Iteration 728, MSE: 0.026158730908698653\n",
      "Iteration 729, MSE: 0.02607127978635468\n",
      "Iteration 730, MSE: 0.02598488796147891\n",
      "Iteration 731, MSE: 0.0258995423100518\n",
      "Iteration 732, MSE: 0.025815229870682896\n",
      "Iteration 733, MSE: 0.025731937842595098\n",
      "Iteration 734, MSE: 0.025649653583635024\n",
      "Iteration 735, MSE: 0.02556836460830759\n",
      "Iteration 736, MSE: 0.02548805858583375\n",
      "Iteration 737, MSE: 0.025408723338234223\n",
      "Iteration 738, MSE: 0.02533034683843548\n",
      "Iteration 739, MSE: 0.025252917208400626\n",
      "Iteration 740, MSE: 0.025176422717281238\n",
      "Iteration 741, MSE: 0.025100851779594386\n",
      "Iteration 742, MSE: 0.025026192953421216\n",
      "Iteration 743, MSE: 0.024952434938627716\n",
      "Iteration 744, MSE: 0.024879566575107703\n",
      "Iteration 745, MSE: 0.02480757684104783\n",
      "Iteration 746, MSE: 0.024736454851213965\n",
      "Iteration 747, MSE: 0.024666189855258498\n",
      "Iteration 748, MSE: 0.02459677123604929\n",
      "Iteration 749, MSE: 0.024528188508017967\n",
      "Iteration 750, MSE: 0.024460431315530648\n",
      "Iteration 751, MSE: 0.024393489431277473\n",
      "Iteration 752, MSE: 0.02432735275468236\n",
      "Iteration 753, MSE: 0.02426201131033263\n",
      "Iteration 754, MSE: 0.024197455246427402\n",
      "Iteration 755, MSE: 0.024133674833247538\n",
      "Iteration 756, MSE: 0.024070660461640383\n",
      "Iteration 757, MSE: 0.024008402641528166\n",
      "Iteration 758, MSE: 0.023946892000430858\n",
      "Iteration 759, MSE: 0.023886119282009233\n",
      "Iteration 760, MSE: 0.023826075344626357\n",
      "Iteration 761, MSE: 0.023766751159925614\n",
      "Iteration 762, MSE: 0.023708137811426652\n",
      "Iteration 763, MSE: 0.023650226493140285\n",
      "Iteration 764, MSE: 0.023593008508197604\n",
      "Iteration 765, MSE: 0.02353647526749918\n",
      "Iteration 766, MSE: 0.02348061828837968\n",
      "Iteration 767, MSE: 0.0234254291932874\n",
      "Iteration 768, MSE: 0.02337089970848356\n",
      "Iteration 769, MSE: 0.023317021662754057\n",
      "Iteration 770, MSE: 0.02326378698614127\n",
      "Iteration 771, MSE: 0.02321118770868757\n",
      "Iteration 772, MSE: 0.023159215959196174\n",
      "Iteration 773, MSE: 0.023107863964008185\n",
      "Iteration 774, MSE: 0.023057124045793815\n",
      "Iteration 775, MSE: 0.02300698862235827\n",
      "Iteration 776, MSE: 0.02295745020546223\n",
      "Iteration 777, MSE: 0.02290850139965959\n",
      "Iteration 778, MSE: 0.022860134901145718\n",
      "Iteration 779, MSE: 0.02281234349662256\n",
      "Iteration 780, MSE: 0.022765120062177003\n",
      "Iteration 781, MSE: 0.022718457562173652\n",
      "Iteration 782, MSE: 0.02267234904815979\n",
      "Iteration 783, MSE: 0.022626787657786734\n",
      "Iteration 784, MSE: 0.022581766613741172\n",
      "Iteration 785, MSE: 0.02253727922269269\n",
      "Iteration 786, MSE: 0.022493318874252616\n",
      "Iteration 787, MSE: 0.02244987903994543\n",
      "Iteration 788, MSE: 0.022406953272195274\n",
      "Iteration 789, MSE: 0.022364535203322586\n",
      "Iteration 790, MSE: 0.02232261854455324\n",
      "Iteration 791, MSE: 0.022281197085043114\n",
      "Iteration 792, MSE: 0.022240264690909617\n",
      "Iteration 793, MSE: 0.02219981530428002\n",
      "Iteration 794, MSE: 0.022159842942348417\n",
      "Iteration 795, MSE: 0.022120341696446662\n",
      "Iteration 796, MSE: 0.022081305731124188\n",
      "Iteration 797, MSE: 0.022042729283242152\n",
      "Iteration 798, MSE: 0.02200460666107637\n",
      "Iteration 799, MSE: 0.021966932243432925\n",
      "Iteration 800, MSE: 0.021929700478773396\n",
      "Iteration 801, MSE: 0.02189290588435318\n",
      "Iteration 802, MSE: 0.02185654304536747\n",
      "Iteration 803, MSE: 0.021820606614110154\n",
      "Iteration 804, MSE: 0.021785091309142215\n",
      "Iteration 805, MSE: 0.021749991914470867\n",
      "Iteration 806, MSE: 0.021715303278737646\n",
      "Iteration 807, MSE: 0.021681020314418525\n",
      "Iteration 808, MSE: 0.021647137997032384\n",
      "Iteration 809, MSE: 0.021613651364359623\n",
      "Iteration 810, MSE: 0.021580555515671052\n",
      "Iteration 811, MSE: 0.021547845610966\n",
      "Iteration 812, MSE: 0.021515516870218947\n",
      "Iteration 813, MSE: 0.021483564572637493\n",
      "Iteration 814, MSE: 0.021451984055927308\n",
      "Iteration 815, MSE: 0.021420770715568065\n",
      "Iteration 816, MSE: 0.021389920004097035\n",
      "Iteration 817, MSE: 0.021359427430401905\n",
      "Iteration 818, MSE: 0.021329288559023146\n",
      "Iteration 819, MSE: 0.021299499009463888\n",
      "Iteration 820, MSE: 0.021270054455508832\n",
      "Iteration 821, MSE: 0.021240950624551478\n",
      "Iteration 822, MSE: 0.021212183296931114\n",
      "Iteration 823, MSE: 0.021183748305274364\n",
      "Iteration 824, MSE: 0.021155641533849082\n",
      "Iteration 825, MSE: 0.021127858917924108\n",
      "Iteration 826, MSE: 0.021100396443137364\n",
      "Iteration 827, MSE: 0.021073250144870378\n",
      "Iteration 828, MSE: 0.021046416107633498\n",
      "Iteration 829, MSE: 0.021019890464456934\n",
      "Iteration 830, MSE: 0.02099366939628858\n",
      "Iteration 831, MSE: 0.02096774913140079\n",
      "Iteration 832, MSE: 0.02094212594480377\n",
      "Iteration 833, MSE: 0.020916796157667023\n",
      "Iteration 834, MSE: 0.020891756136746376\n",
      "Iteration 835, MSE: 0.02086700229381988\n",
      "Iteration 836, MSE: 0.0208425310851294\n",
      "Iteration 837, MSE: 0.020818339010830075\n",
      "Iteration 838, MSE: 0.020794422614445682\n",
      "Iteration 839, MSE: 0.020770778482331786\n",
      "Iteration 840, MSE: 0.02074740324314449\n",
      "Iteration 841, MSE: 0.020724293567316147\n",
      "Iteration 842, MSE: 0.02070144616653854\n",
      "Iteration 843, MSE: 0.02067885779325059\n",
      "Iteration 844, MSE: 0.020656525240133627\n",
      "Iteration 845, MSE: 0.020634445339613665\n",
      "Iteration 846, MSE: 0.020612614963368105\n",
      "Iteration 847, MSE: 0.020591031021838596\n",
      "Iteration 848, MSE: 0.02056969046375264\n",
      "Iteration 849, MSE: 0.020548590275646993\n",
      "Iteration 850, MSE: 0.02052772748140069\n",
      "Iteration 851, MSE: 0.020507099141772202\n",
      "Iteration 852, MSE: 0.020486702353940997\n",
      "Iteration 853, MSE: 0.02046653425105838\n",
      "Iteration 854, MSE: 0.02044659200179982\n",
      "Iteration 855, MSE: 0.02042687280992578\n",
      "Iteration 856, MSE: 0.020407373913847063\n",
      "Iteration 857, MSE: 0.020388092586194635\n",
      "Iteration 858, MSE: 0.020369026133395973\n",
      "Iteration 859, MSE: 0.020350171895256865\n",
      "Iteration 860, MSE: 0.020331527244546697\n",
      "Iteration 861, MSE: 0.020313089586590824\n",
      "Iteration 862, MSE: 0.02029485635886677\n",
      "Iteration 863, MSE: 0.020276825030606106\n",
      "Iteration 864, MSE: 0.020258993102400286\n",
      "Iteration 865, MSE: 0.02024135810581292\n",
      "Iteration 866, MSE: 0.02022391760299544\n",
      "Iteration 867, MSE: 0.02020666918630817\n",
      "Iteration 868, MSE: 0.02018961047794601\n",
      "Iteration 869, MSE: 0.020172739129568712\n",
      "Iteration 870, MSE: 0.020156052821935784\n",
      "Iteration 871, MSE: 0.020139549264545916\n",
      "Iteration 872, MSE: 0.02012322619528069\n",
      "Iteration 873, MSE: 0.020107081380053234\n",
      "Iteration 874, MSE: 0.020091112612460688\n",
      "Iteration 875, MSE: 0.02007531771344027\n",
      "Iteration 876, MSE: 0.020059694530932395\n",
      "Iteration 877, MSE: 0.02004424093954452\n",
      "Iteration 878, MSE: 0.020028954840220943\n",
      "Iteration 879, MSE: 0.020013834159916904\n",
      "Iteration 880, MSE: 0.019998876851275663\n",
      "Iteration 881, MSE: 0.01998408089231112\n",
      "Iteration 882, MSE: 0.019969444286092444\n",
      "Iteration 883, MSE: 0.01995496506043375\n",
      "Iteration 884, MSE: 0.019940641267588266\n",
      "Iteration 885, MSE: 0.0199264709839444\n",
      "Iteration 886, MSE: 0.01991245230972755\n",
      "Iteration 887, MSE: 0.01989858336870396\n",
      "Iteration 888, MSE: 0.019884862307889956\n",
      "Iteration 889, MSE: 0.01987128729726278\n",
      "Iteration 890, MSE: 0.019857856529476792\n",
      "Iteration 891, MSE: 0.019844568219582248\n",
      "Iteration 892, MSE: 0.01983142060474774\n",
      "Iteration 893, MSE: 0.01981841194398615\n",
      "Iteration 894, MSE: 0.019805540517883715\n",
      "Iteration 895, MSE: 0.019792804628333476\n",
      "Iteration 896, MSE: 0.019780202598270277\n",
      "Iteration 897, MSE: 0.01976773277141037\n",
      "Iteration 898, MSE: 0.019755393511994578\n",
      "Iteration 899, MSE: 0.019743183204532392\n",
      "Iteration 900, MSE: 0.01973110025355257\n",
      "Iteration 901, MSE: 0.01971914308335408\n",
      "Iteration 902, MSE: 0.019707310137761124\n",
      "Iteration 903, MSE: 0.019695599879881703\n",
      "Iteration 904, MSE: 0.019684010791867577\n",
      "Iteration 905, MSE: 0.019672541374679907\n",
      "Iteration 906, MSE: 0.019661190147854907\n",
      "Iteration 907, MSE: 0.019649955649273944\n",
      "Iteration 908, MSE: 0.01963883643493667\n",
      "Iteration 909, MSE: 0.01962783107873532\n",
      "Iteration 910, MSE: 0.019616938172234883\n",
      "Iteration 911, MSE: 0.019606156324452506\n",
      "Iteration 912, MSE: 0.019595484161641644\n",
      "Iteration 913, MSE: 0.01958492032707877\n",
      "Iteration 914, MSE: 0.01957446348085225\n",
      "Iteration 915, MSE: 0.01956411229965419\n",
      "Iteration 916, MSE: 0.01955386547657419\n",
      "Iteration 917, MSE: 0.019543721720896634\n",
      "Iteration 918, MSE: 0.01953367975789991\n",
      "Iteration 919, MSE: 0.019523738328658127\n",
      "Iteration 920, MSE: 0.01951389618984543\n",
      "Iteration 921, MSE: 0.019504152113542506\n",
      "Iteration 922, MSE: 0.01949450488704626\n",
      "Iteration 923, MSE: 0.019484953312679975\n",
      "Iteration 924, MSE: 0.019475496207608935\n",
      "Iteration 925, MSE: 0.01946613240365522\n",
      "Iteration 926, MSE: 0.01945686074711599\n",
      "Iteration 927, MSE: 0.01944768009858503\n",
      "Iteration 928, MSE: 0.019438589332774953\n",
      "Iteration 929, MSE: 0.019429587338342425\n",
      "Iteration 930, MSE: 0.01942067301771503\n",
      "Iteration 931, MSE: 0.01941184528692114\n",
      "Iteration 932, MSE: 0.019403103075421115\n",
      "Iteration 933, MSE: 0.01939444532594113\n",
      "Iteration 934, MSE: 0.019385870994308354\n",
      "Iteration 935, MSE: 0.01937737904928871\n",
      "Iteration 936, MSE: 0.019368968472426584\n",
      "Iteration 937, MSE: 0.019360638257887072\n",
      "Iteration 938, MSE: 0.0193523874122986\n",
      "Iteration 939, MSE: 0.019344214954598784\n",
      "Iteration 940, MSE: 0.01933611991588261\n",
      "Iteration 941, MSE: 0.01932810133925058\n",
      "Iteration 942, MSE: 0.01932015827966119\n",
      "Iteration 943, MSE: 0.01931228980378298\n",
      "Iteration 944, MSE: 0.019304494989849968\n",
      "Iteration 945, MSE: 0.019296772927518104\n",
      "Iteration 946, MSE: 0.01928912271772387\n",
      "Iteration 947, MSE: 0.019281543472544637\n",
      "Iteration 948, MSE: 0.019274034315060282\n",
      "Iteration 949, MSE: 0.01926659437921666\n",
      "Iteration 950, MSE: 0.019259222809692046\n",
      "Iteration 951, MSE: 0.01925191876176247\n",
      "Iteration 952, MSE: 0.01924468140117199\n",
      "Iteration 953, MSE: 0.019237509904001898\n",
      "Iteration 954, MSE: 0.01923040345654343\n",
      "Iteration 955, MSE: 0.01922336125517028\n",
      "Iteration 956, MSE: 0.019216382506214737\n",
      "Iteration 957, MSE: 0.019209466425843258\n",
      "Iteration 958, MSE: 0.019202612239935704\n",
      "Iteration 959, MSE: 0.01919581918396354\n",
      "Iteration 960, MSE: 0.019189086502872873\n",
      "Iteration 961, MSE: 0.019182413450964778\n",
      "Iteration 962, MSE: 0.019175799291781846\n",
      "Iteration 963, MSE: 0.0191692432979913\n",
      "Iteration 964, MSE: 0.01916274475127362\n",
      "Iteration 965, MSE: 0.01915630294221009\n",
      "Iteration 966, MSE: 0.01914991717017259\n",
      "Iteration 967, MSE: 0.019143586743215174\n",
      "Iteration 968, MSE: 0.019137310977965474\n",
      "Iteration 969, MSE: 0.019131089199519422\n",
      "Iteration 970, MSE: 0.019124920741335615\n",
      "Iteration 971, MSE: 0.01911880494513241\n",
      "Iteration 972, MSE: 0.019112741160784492\n",
      "Iteration 973, MSE: 0.019106728746223437\n",
      "Iteration 974, MSE: 0.019100767067335992\n",
      "Iteration 975, MSE: 0.019094855497867238\n",
      "Iteration 976, MSE: 0.0190889934193214\n",
      "Iteration 977, MSE: 0.01908318022086759\n",
      "Iteration 978, MSE: 0.01907741529924348\n",
      "Iteration 979, MSE: 0.01907169805866211\n",
      "Iteration 980, MSE: 0.019066027910718843\n",
      "Iteration 981, MSE: 0.019060404274300526\n",
      "Iteration 982, MSE: 0.019054826575494375\n",
      "Iteration 983, MSE: 0.019049294247499366\n",
      "Iteration 984, MSE: 0.01904380673053799\n",
      "Iteration 985, MSE: 0.019038363471768717\n",
      "Iteration 986, MSE: 0.0190329639252005\n",
      "Iteration 987, MSE: 0.019027607551608417\n",
      "Iteration 988, MSE: 0.019022293818448287\n",
      "Iteration 989, MSE: 0.019017022199775437\n",
      "Iteration 990, MSE: 0.01901179217616236\n",
      "Iteration 991, MSE: 0.019006603234617375\n",
      "Iteration 992, MSE: 0.01900145486850582\n",
      "Iteration 993, MSE: 0.018996346577470968\n",
      "Iteration 994, MSE: 0.01899127786735574\n",
      "Iteration 995, MSE: 0.01898624825012631\n",
      "Iteration 996, MSE: 0.01898125724379685\n",
      "Iteration 997, MSE: 0.018976304372352735\n",
      "Iteration 998, MSE: 0.018971389165678965\n",
      "Iteration 999, MSE: 0.018966511159485038\n",
      "Iteration 1000, MSE: 0.018961669895233856\n",
      "Training completed. Final weights: [0.21529239 0.85217692 1.06746931]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m my_reg \u001B[38;5;241m=\u001B[39m LinearRegression()\n\u001B[0;32m      2\u001B[0m my_reg\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m mean_squared_error(y, my_reg\u001B[38;5;241m.\u001B[39mpredict(X)) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1e-3\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou are amazing! Great work!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T07:30:15.631985300Z",
     "start_time": "2023-12-13T07:30:13.101027100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge MSE: 5.207336604954252e-08\n",
      "My Linear Regression MSE: 43406.55521362913\n",
      "Results are different\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.01)\n",
    "ridge_reg.fit(X_scaled, y)\n",
    "ridge_pred = ridge_reg.predict(X_scaled)\n",
    "ridge_mse = mean_squared_error(y, ridge_pred)\n",
    "\n",
    "my_reg = LinearRegression(l_ratio=0.01)\n",
    "my_reg.fit(X_scaled, y)\n",
    "my_pred = my_reg.predict(X_scaled)\n",
    "my_mse = mean_squared_error(y, my_pred)\n",
    "\n",
    "print(f'Ridge MSE: {ridge_mse}')\n",
    "print(f'My Linear Regression MSE: {my_mse}')\n",
    "print(\"Results are similar\" if np.isclose(ridge_mse, my_mse, atol=1e-3) else \"Results are different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
