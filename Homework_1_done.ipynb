{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:49:32.122143300Z",
     "start_time": "2023-12-18T16:49:32.100148800Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:49:33.669961500Z",
     "start_time": "2023-12-18T16:49:33.608871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Создаем синтетический датасет\n",
    "X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n",
    "\n",
    "# Разделяем датасет на тренировочную и тестовую выборки  test_size=0.2 указывает, что 20% данных будут использоваться для тестирования, а остальные 80% - для обучения модели.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:49:35.119807800Z",
     "start_time": "2023-12-18T16:49:35.057293600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "среднеквадратичная ошибка 0.010319177035099037\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем значения для тестовой выборки\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оцениваем качество модели\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'среднеквадратичная ошибка {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:49:54.256978600Z",
     "start_time": "2023-12-18T16:49:54.218993800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "среднеквадратичная ошибка 0.010816091830096245\n"
     ]
    }
   ],
   "source": [
    "# Создаем и обучаем модель с использованием метода градиентного спуска\n",
    "model = SGDRegressor( alpha = 0.0001, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем значения для тестовой выборки\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оцениваем качество модели\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'среднеквадратичная ошибка {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "Эта метрика измеряет, насколько средние квадратичные отклонения предсказанных значений от фактических. Для этой задачи, чем ближе значение MSE к нулю, тем лучше модель.\n",
    "\n",
    "Различие в значениях MSE между LinearRegression и SGDRegressor может быть связано с несколькими факторами. Во-первых, гиперпараметр alpha в SGDRegressor отвечает за силу регуляризации, и его неоптимальное значение может влиять на результаты. Во-вторых, стохастический характер SGD и неявное указание количества итераций могут привести к случайным колебаниям в результатах.\n",
    "\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression.\n",
    "\n",
    "Для подбора гиперпараметров в методе градиентного спуска (SGDRegressor), мы можем использовать процесс подбора по сетке (Grid Search), который позволяет перебирать различные комбинации значений гиперпараметров.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T16:50:03.696248100Z",
     "start_time": "2023-12-18T16:49:56.575475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SGDRegressor MSE: 0.01053810256554833 with hyperparameters: {'alpha': 0.0001, 'learning_rate': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Определяем пространство гиперпараметров для SGDRegressor\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Создаем модель SGDRegressor\n",
    "sgd_model = SGDRegressor()\n",
    "\n",
    "# Инициируем GridSearchCV\n",
    "grid_search = GridSearchCV(sgd_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Получаем лучшие гиперпараметры\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Используем лучшие гиперпараметры для обучения модели\n",
    "best_sgd_model = SGDRegressor(alpha=best_params['alpha'], learning_rate=best_params['learning_rate'])\n",
    "best_sgd_model.fit(X_train, y_train)\n",
    "sgd_pred = best_sgd_model.predict(X_test)\n",
    "sgd_mse = mean_squared_error(y_test, sgd_pred)\n",
    "\n",
    "# Выводим результаты\n",
    "print(f'Best SGDRegressor MSE: {sgd_mse} with hyperparameters: {best_params}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:50:23.530462100Z",
     "start_time": "2023-12-18T16:50:23.455821800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not converge: Max iterations reached.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (20,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[117], line 75\u001B[0m\n\u001B[0;32m     72\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# Предсказываем значения для тестовой выборки\u001B[39;00m\n\u001B[1;32m---> 75\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# Оцениваем качество модели\u001B[39;00m\n\u001B[0;32m     78\u001B[0m mse \u001B[38;5;241m=\u001B[39m mean_squared_error(y_test, y_pred)\n",
      "Cell \u001B[1;32mIn[117], line 60\u001B[0m, in \u001B[0;36mMyLinearRegression.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Добавляем столбец единиц для учета свободного члена (intercept)\u001B[39;00m\n\u001B[0;32m     59\u001B[0m X_with_intercept \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack((np\u001B[38;5;241m.\u001B[39mones((X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m)), X))\n\u001B[1;32m---> 60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_with_intercept\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: shapes (20,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iters=1000, tol_weight_change=1e-4, tol_mse_change=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.tol_weight_change = tol_weight_change\n",
    "        self.tol_mse_change = tol_mse_change\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Number of samples in X and y must be the same.\")\n",
    "        \n",
    "        # Добавляем столбец единиц для учета свободного члена (intercept)\n",
    "        X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        # Инициализируем веса случайным образом\n",
    "        self.weights = np.random.rand(X_with_intercept.shape[1])\n",
    "        \n",
    "        for _ in range(self.max_iters):\n",
    "            # Вычисляем предсказания модели\n",
    "            predictions = np.dot(X_with_intercept, self.weights)\n",
    "            \n",
    "            # Вычисляем ошибку предсказания\n",
    "            errors = predictions - y\n",
    "            \n",
    "            # Вычисляем градиент функции потерь (среднеквадратичной ошибки)\n",
    "            gradient = np.dot(X_with_intercept.T, errors) / len(y)\n",
    "            \n",
    "            # Обновляем веса с учетом градиента и скорости обучения\n",
    "            new_weights = self.weights - self.learning_rate * gradient\n",
    "            \n",
    "            # Проверяем критерии останова\n",
    "            if np.linalg.norm(new_weights - self.weights) < self.tol_weight_change:\n",
    "                print(\"Converged: Change in weights is below tolerance.\")\n",
    "                break\n",
    "            \n",
    "            new_predictions = np.dot(X_with_intercept, new_weights)\n",
    "            mse_change = np.abs(np.mean((new_predictions - y) ** 2) - np.mean((predictions - y) ** 2))\n",
    "            \n",
    "            if mse_change < self.tol_mse_change:\n",
    "                print(\"Converged: Change in MSE is below tolerance.\")\n",
    "                break\n",
    "            \n",
    "            # Обновляем веса\n",
    "            self.weights = new_weights\n",
    "        \n",
    "        else:\n",
    "            print(\"Did not converge: Max iterations reached.\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if X.shape[1] + 1 != len(self.weights):\n",
    "            raise ValueError(\"Number of features in X must match the number of weights.\")\n",
    "        \n",
    "        # Добавляем столбец единиц для учета свободного члена (intercept)\n",
    "        X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        return np.dot(X_with_intercept, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "# Пример использования:\n",
    "# Создаем случайные данные для демонстрации\n",
    "X = np.random.rand(100, 2)\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + 2 + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "model = MyLinearRegression(learning_rate=0.01, max_iters=1000, tol_weight_change=1e-4, tol_mse_change=1e-4)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказываем значения для тестовой выборки\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Оцениваем качество модели\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'MyLinearRegression MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:50:26.459992900Z",
     "start_time": "2023-12-18T16:50:26.438475800Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LinearRegression.__init__() got an unexpected keyword argument 'regularization'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[118], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m my_reg \u001B[38;5;241m=\u001B[39m \u001B[43mLinearRegression\u001B[49m\u001B[43m(\u001B[49m\u001B[43mregularization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ml2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m my_reg\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[0;32m      3\u001B[0m predictions \u001B[38;5;241m=\u001B[39m my_reg\u001B[38;5;241m.\u001B[39mpredict(X)\n",
      "\u001B[1;31mTypeError\u001B[0m: LinearRegression.__init__() got an unexpected keyword argument 'regularization'"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression(regularization='l2')\n",
    "my_reg.fit(X, y)\n",
    "predictions = my_reg.predict(X)\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T16:50:28.169202500Z",
     "start_time": "2023-12-18T16:50:28.051860600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (alpha=0.1) MSE: 0.09844461927035866\n",
      "Lasso (alpha=0.01) MSE: 0.0009842001698743407\n",
      "Ridge (alpha=0.1) MSE: 5.642771301488967e-06\n",
      "Ridge (alpha=0.01) MSE: 5.642904441531955e-08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "# Разделяем данные на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Задаем значения параметра регуляризации\n",
    "alpha_l1_1 = 0.1\n",
    "alpha_l1_2 = 0.01\n",
    "alpha_l2_1 = 0.1\n",
    "alpha_l2_2 = 0.01\n",
    "\n",
    "# Обучаем модели с l1-регуляризацией\n",
    "lasso_model_1 = Lasso(alpha=alpha_l1_1)\n",
    "lasso_model_2 = Lasso(alpha=alpha_l1_2)\n",
    "\n",
    "lasso_model_1.fit(X_train, y_train)\n",
    "lasso_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Обучаем модели с l2-регуляризацией\n",
    "ridge_model_1 = Ridge(alpha=alpha_l2_1)\n",
    "ridge_model_2 = Ridge(alpha=alpha_l2_2)\n",
    "\n",
    "ridge_model_1.fit(X_train, y_train)\n",
    "ridge_model_2.fit(X_train, y_train)\n",
    "\n",
    "# Делаем предсказания\n",
    "y_pred_lasso_1 = lasso_model_1.predict(X_test)\n",
    "y_pred_lasso_2 = lasso_model_2.predict(X_test)\n",
    "\n",
    "y_pred_ridge_1 = ridge_model_1.predict(X_test)\n",
    "y_pred_ridge_2 = ridge_model_2.predict(X_test)\n",
    "\n",
    "# Оцениваем качество моделей\n",
    "mse_lasso_1 = mean_squared_error(y_test, y_pred_lasso_1)\n",
    "mse_lasso_2 = mean_squared_error(y_test, y_pred_lasso_2)\n",
    "\n",
    "mse_ridge_1 = mean_squared_error(y_test, y_pred_ridge_1)\n",
    "mse_ridge_2 = mean_squared_error(y_test, y_pred_ridge_2)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Lasso (alpha=0.1) MSE:\", mse_lasso_1)\n",
    "print(\"Lasso (alpha=0.01) MSE:\", mse_lasso_2)\n",
    "print(\"Ridge (alpha=0.1) MSE:\", mse_ridge_1)\n",
    "print(\"Ridge (alpha=0.01) MSE:\", mse_ridge_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
