{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №1: линейная регрессия и векторное дифференцирование (10 баллов).\n",
    "\n",
    "* Максимальное количество баллов за задания в ноутбуке - 11, но больше 10 оценка не ставится, поэтому для получения максимальной оценки можно сделать не все задания.\n",
    "\n",
    "* Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:25:26.696461100Z",
     "start_time": "2023-12-17T15:25:26.439547800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:37:15.024100Z",
     "start_time": "2023-12-17T15:37:14.939523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1468414548604703e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T15:31:21.823453900Z",
     "start_time": "2023-12-17T15:31:21.775003600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.046723418584293e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([ 1.90033162e-09,  1.96433381e-08,  9.26628677e-09, -3.84773075e-08,\n        1.47468710e-08,  1.21494024e-08,  6.77723292e+01, -3.49158196e-09,\n        2.37109423e-08, -1.34518594e-08, -2.58369292e-08,  1.73055404e-09,\n        3.65323841e-08,  7.43943475e-09, -4.23991957e-08,  5.38195179e-08,\n        3.95043564e-08, -1.70654302e-08,  1.92318400e-08,  1.17472318e-08,\n        3.23095690e+01, -2.21359925e-08,  5.54880731e+01,  1.34822735e-08,\n        6.33770148e+01, -2.09396500e-08, -3.73313095e-08,  1.24049117e-08,\n       -1.24620685e-08, -1.79478247e-08, -8.20889600e-09,  2.49714742e-08,\n       -2.37686128e-08, -5.01524779e-09,  2.03728415e-08, -1.93613874e-08,\n       -7.75245694e-09, -2.20035285e-08,  2.26384509e-09,  2.15849256e-08,\n       -1.16812989e-08,  3.04058699e-08, -2.82813795e-08, -2.55107268e-08,\n       -3.86227893e-08, -6.18340838e-09,  7.46998230e-10, -7.13338225e-09,\n       -2.87097296e-08, -4.99705783e-08,  2.73930283e-08,  2.74141756e+01,\n       -4.55785374e-08,  3.35113570e-08, -4.35375010e-09,  3.11412875e-08,\n        3.08465278e-08,  2.27463564e-08, -5.19139193e-09, -1.49231999e-09,\n        4.54568849e-09,  1.22318718e-08,  3.55874356e-08,  2.21789109e-09,\n        4.46813477e-09,  1.55953409e+01,  4.10375150e-09, -3.23483544e-08,\n        2.21809103e-08, -6.46805274e-09, -1.87918816e-08,  8.89866356e-09,\n        1.40398174e-08,  2.11264450e-08,  4.46637205e-08, -3.44487072e-08,\n        2.20461701e-09, -2.05803650e-08,  3.57783372e-09,  5.32723013e+01,\n       -2.23859567e-08,  1.47325588e+01,  2.26183238e-09, -4.17792493e-08,\n       -3.90147497e-09, -2.13212765e-08, -3.31122050e-08,  2.69295462e-08,\n       -3.78673939e-08,  1.84603900e-09,  3.98841049e-08,  3.81535622e-08,\n        1.51410969e-08,  5.43900179e+01,  1.17168937e-08,  2.55634252e+01,\n       -1.66939644e-08, -3.07131080e-08, -2.67075430e-08, -3.10999592e-08])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (0.5 балла).*** Объясните, чем вызвано различие двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (0.5 балла).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и второй варианты) или l2 (третий и четвертый варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:53:56.798156300Z",
     "start_time": "2023-12-17T19:53:55.459349500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azhoh\\PycharmProjects\\homeWork2\\.venv\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\azhoh\\AppData\\Local\\Temp\\ipykernel_17700\\3471345814.py:31: RuntimeWarning: overflow encountered in square\n",
      "  mse_diff = np.mean((y_pred - y) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, alpha=0.0001, l_ratio=0.001, tol=0.001, max_iter=1000):\n",
    "        self.alpha = alpha\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "\n",
    "        prev_weights = None\n",
    "        iter_count = 0\n",
    "        while True:\n",
    "            y_pred = np.dot(X, self.weights)\n",
    "            error = y_pred - y\n",
    "            gradient = np.dot(X.T, error)\n",
    "\n",
    "            if self.l_ratio > 0:\n",
    "                regularization = self.l_ratio * np.sign(self.weights)\n",
    "                gradient += regularization\n",
    "\n",
    "            self.weights -= self.alpha * gradient\n",
    "\n",
    "            if prev_weights is not None:\n",
    "                weight_diff = np.linalg.norm(self.weights - prev_weights)\n",
    "                mse_diff = np.mean((y_pred - y) ** 2)\n",
    "                if weight_diff < self.tol or mse_diff < self.tol:\n",
    "                    break\n",
    "\n",
    "            prev_weights = np.copy(self.weights)\n",
    "            iter_count += 1\n",
    "\n",
    "            if iter_count >= self.max_iter:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        y_pred = np.dot(X, self.weights)\n",
    "        return y_pred\n",
    "\n",
    "# Генерация случайных данных\n",
    "X, y = make_regression(n_samples=10000, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Создание объекта линейной регрессии\n",
    "my_reg = LinearRegression(alpha=0.01, l_ratio=0.01, tol=0.0001, max_iter=10000)\n",
    "\n",
    "# Обучение модели\n",
    "my_reg.fit(X, y)\n",
    "\n",
    "# Предсказание на тех же данных\n",
    "predictions = my_reg.predict(X)\n",
    "\n",
    "# Печать предсказаний\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:54:08.205023700Z",
     "start_time": "2023-12-17T19:54:08.175165500Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m my_reg \u001B[38;5;241m=\u001B[39m LinearRegression()\n\u001B[0;32m      2\u001B[0m my_reg\u001B[38;5;241m.\u001B[39mfit(X, y)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m mean_squared_error(y, my_reg\u001B[38;5;241m.\u001B[39mpredict(X)) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1e-3\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mYou are amazing! Great work!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки\n",
    "\n",
    "* с l1-регуляризацией (from sklearn.linear_model import Lasso, **первый и второй вариант**) или с l2-регуляризацией (from sklearn.linear_model import Ridge, **третий и четвертый вариант**)\n",
    "* со значением параметра регуляризации **0.1 - для первого и третьего варианта, 0.01 - для второго и четвертого варианта**. \n",
    "\n",
    "Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T19:56:04.101380700Z",
     "start_time": "2023-12-17T19:56:04.003575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Mean Squared Error: 0.10479288650494291\n",
      "Ridge Mean Squared Error: 0.009986319700053835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 6* (1 балл).***\n",
    "Пусть $P, Q \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_Q tr(PQ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 7* (1 балл).***\n",
    "Пусть $x, y \\in \\mathbb{R}^{n}, M \\in \\mathbb{R}^{n\\times n}$. Найдите $\\nabla_M x^T M y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решения заданий 6 и 7 можно написать на листочке и отправить в anytask вместе с заполненным ноутбуком."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
